import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, MinMaxScaler
from deepctr.models import WDL, DeepFM, DIN
from deepctr.feature_column import SparseFeat, DenseFeat, VarLenSparseFeat, get_feature_names
import tensorflow as tf
from tqdm import tqdm
import sys


RANDOM_SEED = 42
np.random.seed(RANDOM_SEED)
tf.random.set_seed(RANDOM_SEED)

gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    try:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
    except RuntimeError as e:
        print(f"Error configuring GPU memory growth: {e}")


class TqdmCallback(tf.keras.callbacks.Callback):
    def __init__(self, epochs):
        self.epochs = epochs
        self.epoch_bar = None
        self.batch_bar = None

    def on_train_begin(self, logs=None):
        print("\nè®­ç»ƒå¼€å§‹...")
        self.epoch_bar = tqdm(total=self.epochs, desc="Epoch", position=0, leave=True)

    def on_epoch_begin(self, epoch, logs=None):
        if self.batch_bar:
            self.batch_bar.close()
        self.current_epoch = epoch
        self.batch_bar = None

    def on_batch_begin(self, batch, logs=None):
        if not self.batch_bar:
            total_batches = self.params['steps'] if self.params['steps'] else None
            self.batch_bar = tqdm(
                total=total_batches,
                desc=f"Epoch {self.current_epoch + 1}/{self.epochs}",
                position=1,
                leave=False
            )
        self.batch_bar.update(1)

    def on_batch_end(self, batch, logs=None):
        logs = logs or {}
        loss = logs.get('loss', float('nan'))
        auc = logs.get('auc', float('nan'))
        self.batch_bar.set_postfix({
            'loss': f"{loss:.4f}",
            'auc': f"{auc:.4f}"
        })

    def on_epoch_end(self, epoch, logs=None):
        logs = logs or {}
        val_loss = logs.get('val_loss', float('nan'))
        val_auc = logs.get('val_auc', float('nan'))
        self.epoch_bar.set_postfix({
            'val_loss': f"{val_loss:.4f}",
            'val_auc': f"{val_auc:.4f}"
        })
        self.epoch_bar.update(1)

    def on_train_end(self, logs=None):
        if self.batch_bar:
            self.batch_bar.close()
        self.epoch_bar.close()
        print("è®­ç»ƒå®Œæˆï¼")


def preprocess_movielens(data_path='./achive'):
    print("\nğŸ”„ å¼€å§‹æ•°æ®é¢„å¤„ç†...")

    print("ğŸ“– 1/7: è¯»å–æ•°æ®...")
    ratings = pd.read_csv(f'{data_path}/rating.csv')

    #è¿™é‡Œå¯ä»¥è®¾ç½®æ•°æ®é‡çš„å¤§å°ï¼Œmovielensæ•°æ®é›†å…±ä¸¤åƒå¤šä¸‡æ¡ï¼Œè¿™é‡Œæˆ‘ä»¬é»˜è®¤å–æœ€æ–°çš„5wæ¡
    print("â³ æŒ‰æ—¶é—´æ’åºå¹¶é€‰å–æœ€æ–°çš„5wæ¡è®°å½•...")
    ratings = ratings.sort_values('timestamp', ascending=False).head(50000)
    ratings = ratings.reset_index(drop=True)

    print(f"å·²é€‰å–æœ€æ–°çš„ {len(ratings)} æ¡è®°å½•")
    movies = pd.read_csv(f'{data_path}/movie.csv')

    print("ğŸ”€ 2/7: åˆå¹¶ç”µå½±ç±»åˆ«ä¿¡æ¯...")
    ratings = ratings.merge(movies[['movieId', 'genres']], on='movieId', how='left')


    print("ğŸ” 3/7: æ„é€ æ­£è´Ÿæ ·æœ¬...")
    ratings['label'] = (ratings['rating'] >= 4).astype(int)
    ratings = ratings[['userId', 'movieId', 'genres', 'label']]


    print("âœ‚ï¸ åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†...")
    train, test = train_test_split(ratings, test_size=0.2, random_state=RANDOM_SEED)


    print("ğŸ”¢ 5/7: ç¦»æ•£ç‰¹å¾ç¼–ç ...")
    for feat in tqdm(['userId', 'movieId'], desc="ç¼–ç è¿›åº¦"):
        le = LabelEncoder()
        train[feat] = le.fit_transform(train[feat])


        test[feat] = test[feat].map(lambda x: le.transform([x])[0] if x in le.classes_ else -1)
        test[feat] = test[feat].replace(-1, 0)  # å°†æœªè§è¿‡çš„å€¼æ˜ å°„ä¸º0


    print("ğŸ“Š 6/7: å¤„ç†å¤šå€¼ç±»åˆ«ç‰¹å¾...")


    first_genres = []
    for s in tqdm(train['genres'].str.split('|'), desc="æå–ç±»åˆ«è¿›åº¦", total=len(train)):
        if s and len(s) > 0:
            first_genres.append(s[0])
        else:
            first_genres.append('')

    unique_first_genres = list(set(first_genres))
    genre_encoder = {g: i + 1 for i, g in enumerate(unique_first_genres)}


    def encode_first_genre(x):
        if isinstance(x, str):
            genres = x.split('|')
            if genres and len(genres) > 0:
                return genre_encoder.get(genres[0], 0)  # è¿”å›ç¬¬ä¸€ä¸ªç±»åˆ«ç¼–ç ï¼Œä¸å­˜åœ¨åˆ™ä¸º0
        return 0

    train['genres'] = [encode_first_genre(x) for x in tqdm(train['genres'], desc="ç¼–ç è®­ç»ƒé›†ç±»åˆ«è¿›åº¦")]
    test['genres'] = [encode_first_genre(x) for x in tqdm(test['genres'], desc="ç¼–ç æµ‹è¯•é›†ç±»åˆ«è¿›åº¦")]


    print("ğŸ”¢ 7/7: ç¡®ä¿æ‰€æœ‰ç‰¹å¾éƒ½æ˜¯æ•°å€¼ç±»å‹...")
    for col in ['userId', 'movieId', 'label', 'genres']:
        train[col] = pd.to_numeric(train[col], errors='coerce')
        test[col] = pd.to_numeric(test[col], errors='coerce')


    train = train.fillna(0)
    test = test.fillna(0)


    print("ğŸ•’ 8/8: æ„å»ºç”¨æˆ·å†å²è¡Œä¸ºåºåˆ—...")
    user_history = train[['userId', 'movieId', 'label']].sort_values('userId')
    user_item_dict = user_history[user_history['label'] == 1].groupby('userId')['movieId'].apply(list).to_dict()


    print("ğŸ¯ 9/9: ç”Ÿæˆå¸¦å†å²åºåˆ—çš„æ ·æœ¬...")

    def generate_samples(df):
        samples = []
        for idx, row in tqdm(df.iterrows(), desc="ç”Ÿæˆæ ·æœ¬è¿›åº¦", total=len(df)):
            user_id = row['userId']
            hist = user_item_dict.get(user_id, [])

            # é»˜è®¤è´Ÿæ ·æœ¬è‡³å°‘æœ‰1ä¸ªï¼Œå¯è‡ªè¡Œä¿®æ”¹
            if len(hist) == 0 and row['label'] == 0:
                continue

            # é»˜è®¤ä½¿ç”¨10æ¡ç”¨æˆ·è¡Œä¸ºåºåˆ—ï¼Œå¯è‡ªè¡Œä¿®æ”¹
            hist = hist[-10:]
            hist = [0] * (10 - len(hist)) + hist

            samples.append({
                'userId': user_id,
                'movieId': row['movieId'],
                'genres': row['genres'],
                'hist_movieId': hist,
                'label': row['label']
            })
        return pd.DataFrame(samples)

    train = generate_samples(train)
    test = generate_samples(test)

    print("âœ… æ•°æ®é¢„å¤„ç†å®Œæˆï¼")
    return train, test, unique_first_genres



def get_feature_columns(train, test, unique_genres):
    embedding_dim = 8

    user_feature = SparseFeat(
        'userId',
        vocabulary_size=train['userId'].max() + 1,
        embedding_dim=embedding_dim
    )

    movie_feature = SparseFeat(
        'movieId',
        vocabulary_size=train['movieId'].max() + 1,
        embedding_dim=embedding_dim
    )

    genre_feature = SparseFeat(
        'genres',
        vocabulary_size=train['genres'].max() + 1,
        embedding_dim=embedding_dim  # ä¿®æ”¹ä¸ºä¸å…¶ä»–ç‰¹å¾ç›¸åŒçš„ç»´åº¦
    )


    history_feature = VarLenSparseFeat(
        SparseFeat('hist_movieId', vocabulary_size=train['movieId'].max() + 1, embedding_dim=embedding_dim),
        maxlen=10,
        combiner='mean'
    )

    sparse_features = [user_feature, movie_feature, genre_feature]
    sequence_features = [history_feature]
    wide_features = [user_feature, movie_feature, genre_feature]
    deep_features = sparse_features + sequence_features

    return wide_features, deep_features, sparse_features, sequence_features



def train_and_evaluate(model_name, feature_columns, train, test, history_feature=None):
    print("\nğŸ” éªŒè¯è¾“å…¥æ•°æ®...")


    for col in ['userId', 'movieId', 'genres']:
        print(f"åˆ—å: {col}")
        print(f"  ç±»å‹: {train[col].dtype}")
        print(f"  è®­ç»ƒé›†æœ€å°å€¼: {train[col].min()}")
        print(f"  è®­ç»ƒé›†æœ€å¤§å€¼: {train[col].max()}")
        print(f"  æµ‹è¯•é›†æœ€å°å€¼: {test[col].min()}")
        print(f"  æµ‹è¯•é›†æœ€å¤§å€¼: {test[col].max()}")
        print(f"  æ ·æœ¬å€¼: {train[col].iloc[0]}")


    if 'hist_movieId' in train:
        hist_lengths = [len(h) for h in train['hist_movieId']]
        print(f"å†å²åºåˆ—é•¿åº¦ - æœ€å°: {min(hist_lengths)}, æœ€å¤§: {max(hist_lengths)}, å¹³å‡: {np.mean(hist_lengths):.2f}")


    train_model_input = {name: train[name] for name in get_feature_names(feature_columns)}
    test_model_input = {name: test[name] for name in get_feature_names(feature_columns)}

    if 'hist_movieId' in train_model_input:
        train_model_input['hist_movieId'] = np.array(train_model_input['hist_movieId'].tolist())
        test_model_input['hist_movieId'] = np.array(test_model_input['hist_movieId'].tolist())

    print("\næ ·æœ¬è¾“å…¥:")
    for name, value in list(train_model_input.items())[:3]:
        print(f"  {name}: {value.shape if hasattr(value, 'shape') else len(value)}")

    y_train = train['label'].values
    y_test = test['label'].values

    if model_name == 'WideDeep':
        model = WDL(linear_feature_columns=feature_columns, dnn_feature_columns=feature_columns)
    elif model_name == 'DeepFM':
        model = DeepFM(linear_feature_columns=feature_columns, dnn_feature_columns=feature_columns)
    elif model_name == 'DIN':
        sparse_features = [feat for feat in feature_columns if isinstance(feat, SparseFeat)]
        sequence_features = [feat for feat in feature_columns if isinstance(feat, VarLenSparseFeat)]

        model = DIN(
            dnn_feature_columns=sparse_features + sequence_features,
            history_feature_list=['movieId'],
            dnn_use_bn=True,
            dnn_hidden_units=(200, 80),
            dnn_activation='dice',
            att_hidden_size=(80, 40),
            att_activation='dice',
            l2_reg_dnn=0.0,
            l2_reg_embedding=1e-6,
            dnn_dropout=0.5,
            seed=RANDOM_SEED,
            task='binary'
        )
    else:
        raise ValueError("Model not supported")

    model.compile(
        optimizer='adam',
        loss='binary_crossentropy',
        metrics=[tf.keras.metrics.AUC(name='auc')]
    )


    epochs = 10
    history = model.fit(
        train_model_input, y_train,
        batch_size=256,
        epochs=epochs,
        validation_data=(test_model_input, y_test),
        verbose=0,
        callbacks=[TqdmCallback(epochs=epochs)]
    )

    test_auc = model.evaluate(test_model_input, y_test, verbose=0)[1]
    print(f"âœ… {model_name} Test AUC: {test_auc:.4f}")
    return model, history


#å¼ºçƒˆå»ºè®®è¿™é‡Œä¸€æ­¥æ­¥æ‰“æ–­ç‚¹æ“ä½œ
if __name__ == "__main__":
    # 1. æ•°æ®é¢„å¤„ç†
    train, test, unique_genres = preprocess_movielens()

    # 2. ç‰¹å¾å·¥ç¨‹
    print("\nğŸ”„ å¼€å§‹ç‰¹å¾å·¥ç¨‹...")
    wide_features, deep_features, sparse_features, sequence_features = get_feature_columns(
        train, test, unique_genres
    )
    history_feature = sequence_features[0]
    print("âœ… ç‰¹å¾å·¥ç¨‹å®Œæˆï¼")

    # 3. å®šä¹‰è¦è®­ç»ƒçš„æ¨¡å‹åˆ—è¡¨
    models = ['WideDeep', 'DeepFM', 'DIN']

    # 4. éå†è®­ç»ƒæ¨¡å‹ï¼ˆå¸¦æ€»ä½“è¿›åº¦æ¡ï¼‰
    print("\nğŸš€ å¼€å§‹æ¨¡å‹è®­ç»ƒ...")
    for model_name in tqdm(models, desc="æ€»ä½“è¿›åº¦"):
        print(f"\nğŸ”„ Training {model_name}...")
        if model_name == 'DIN':
            _, _ = train_and_evaluate(
                model_name,
                feature_columns=sparse_features + sequence_features,
                train=train,
                test=test,
                history_feature=history_feature
            )
        else:
            if model_name == 'WideDeep':
                feature_columns = wide_features + deep_features
            else:
                feature_columns = sparse_features
            _, _ = train_and_evaluate(model_name, feature_columns, train, test)

    print("\nğŸ‰ æ‰€æœ‰æ¨¡å‹è®­ç»ƒå®Œæˆï¼")